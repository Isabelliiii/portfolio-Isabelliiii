---
title: "Week 11 Notes – Classification, ROC & Decision Thresholds"
date: "2025-11-17"
---

## Key Concepts Learned
- Difference between **regression vs. classification**; logistic regression predicts *probabilities*, not categories.
- **Confusion matrix**: TP, FP, TN, FN; each cell has different policy implications.
- **Sensitivity (Recall), Specificity, Precision** — why no single metric is “best.”
- **ROC curve** and **AUC** as global, threshold-free performance measures.
- The idea of **decision thresholds**: model predicts probabilities, humans decide the cutoff.
- **Base rates & class imbalance**: high accuracy ≠ good model if minority class is important.
- **Cost-sensitive evaluation**: false positives vs. false negatives have asymmetric impacts in policy.

## Coding Techniques
- Using `predict(..., type = "response")` to get predicted probabilities.
- Generating a confusion matrix with `caret::confusionMatrix()` or manual table.
- Plotting ROC curves with `pROC::roc()` and extracting **AUC**.
- Evaluating model metrics across multiple thresholds (loop / tibble workflows).
- Using `yardstick`-style metrics: accuracy, sensitivity, specificity, precision, recall, F1.

## Questions & Challenges
- Still clarifying: how to pick an “optimal” threshold when costs are not formally quantified.
- Need more practice interpreting ROC curves beyond “higher AUC is better.”
- Uncertainty about how to adjust thresholds in practice when policy stakes differ.
- Want more examples of **class imbalance handling** (weighting, resampling, etc.).

## Connections to Policy
- Threshold choices directly determine **resource allocation** (e.g., inspectors, fire trucks, case managers).
- False negatives and false positives often imply **very different real-world risks**.
- ROC/AUC help compare models, but **policy choice** determines threshold, not statistics.
- Classification models support **risk-based decision systems**, but need governance to avoid inequity.

## Reflection
- Most interesting: realizing classification is fundamentally about **managing tradeoffs**, not just accuracy.
- The ROC + threshold framework clarified why one model can be used in **multiple policy scenarios**.
- I can apply this by being explicit about:
  - what *policy outcome* matters,
  - which errors are most costly,
  - and selecting thresholds that align with those values.
